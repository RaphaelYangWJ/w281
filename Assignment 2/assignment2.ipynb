{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Imaging Pipeline\n",
    "\n",
    "MIDS W281: Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T02:13:26.316189300Z",
     "start_time": "2024-01-21T02:13:25.729374Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Softwares\\Anaconda\\envs\\Data_Science\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "F:\\Softwares\\Anaconda\\envs\\Data_Science\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "F:\\Softwares\\Anaconda\\envs\\Data_Science\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Demosaicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bayer Pattern](images/bayer_pattern.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "In this exercise you will convert a raw sensor image into a full color image using demosaicing. Digital sensors record color images through a Bayer mosaic (above), where each pixel records only one of the three color colors (RGB). A software interpolation is then needed to reconstruct all three colors at each pixel.\n",
    "\n",
    "**HINT:** There are different Bayer mosaic patterns than the one shown above.  You should look into them and try them out.\n",
    "\n",
    "### Description: \n",
    "We will provide you with some raw images, represented as grayscale images (red, green, and blue pixels are all on the same channel of the image). Your task is to write some python code to demosaic and generate a full three-channel RGB image. You're encouraged to debug your code using the image `signs-small.png` because it is not very large and exhibits some interesting challenges of demosaicing. Note that you may need to convert images between `UINT8` and `float32` data types for computation and visualization.\n",
    "\n",
    "For simplicity we will ignore the pixels at the boundary of the image, specifically the first and last two rows and columns don't need to be reconstructed. This will allow you to focus on the general case and not worry about whether neighboring values are unavailable. It's actually not uncommon for cameras and software to return a slightly-cropped image for similar reasons. Therefore, for an image of size NxN, you will return a cropped image of size (N-2)x(N-2)x3.  \n",
    "\n",
    "1. Write a python function that takes as input a raw image and offset and returns a single-channel 2-D image corresponding to the interpolated green channel. The offset encodes whether either the top-left pixel or its right neighbor is the first green pixel. In our case (Figure 1) the second pixel is green, so offset=1.  \n",
    "\n",
    "2. Write another python function for generating the red and blue channels. This function takes a raw image and two offsets: one for row offset and one for column offset, and returns a single-channel image. The row/column offset for the red channel is (0,0) and for the blue channel (1,1) (see Figure 1). Note that the interpolation for the red/blue channel will be different than the green channel because the recorded pixels are sparser. For interpolated pixels that have two direct neighbors that are known (left-right or up-down), simply take the linear average between the two values. For the remaining case, average the four diagonal pixels. You can ignore the first and last two rows or columns to make sure that you have all the neighbors you need. Similar to the green-channel, interpolate the values when they are missing and copy the values when they are available.  \n",
    "\n",
    "3. Using the above two functions, create a full three-channel, RGB image. You might observe some checkerboard artifacts around strong edges. This is expected from our naive interpolation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables:\n",
    "- Python code for interpolation of green channel and red/blue channel\n",
    "- Full-three channel RGB image for `signs.png` (**THE BIG ONE, NOT THE SMALL ONE**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T07:36:29.387425800Z",
     "start_time": "2024-01-21T07:36:28.994440400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m redblue_img\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# load images\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m test_img \u001B[38;5;241m=\u001B[39m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39mimread(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./demosaicing/signs-small.png\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      9\u001B[0m raw_img \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mimread(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./demosaicing/signs.png\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     11\u001B[0m img \u001B[38;5;241m=\u001B[39m test_img\u001B[38;5;241m.\u001B[39mcopy()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def interpolate_green(raw_img, offset_value):\n",
    "    return green_img\n",
    "\n",
    "def interpolate_redblue(raw_img, offset_pair):\n",
    "    return redblue_img\n",
    "\n",
    "# load images\n",
    "test_img = plt.imread('./demosaicing/signs-small.png')\n",
    "raw_img = plt.imread('./demosaicing/signs.png')\n",
    "\n",
    "img = test_img.copy()\n",
    "# TODO: check img data type\n",
    "\n",
    "# green_offset = ???\n",
    "green_img = interpolate_green(img, green_offset)\n",
    "\n",
    "# blue_offset = ???\n",
    "blue_img = interpolate_redblue(img, blue_offset)\n",
    "\n",
    "# red_offset = ???\n",
    "red_img = interpolate_redblue(img, red_offset)\n",
    "\n",
    "# color_img = ???\n",
    "plt.imshow(color_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Denoising Teaser](images/denoising.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Random noise is a problem that often arises in cameras specially in extremely low light conditions, and its presence can seriously degrade the quality of a digital image. To remedy the situation, an average of multiple images, captured very close in time, can be used to improve the quality final image. Because the camera may move while recording mulitple images, we will need to align the images before averaging and denoising. We will implement this alignment + denoising algorithm.\n",
    "\n",
    "### Description: \n",
    "We provide 18 images captured in a low light setting. One of the images is shown above.\n",
    "\n",
    "Each image is slightly mis-aligned from the previous image in the sequence. Our goal is to align each of the images in the sequence to the first image and then average the aligned images to reduce the noise (which tends to be independent across images). Note that you may need to convert images between `UINT8` and `float32` data types for computation and visualization.\n",
    "\n",
    "1. Write a python function that takes as input two images and returns the horizontal and vertical offset that best aligns the two images. Ignore the difference for all the pixels less than or equal to a `maxOffset` away from the edges. Use a brute force approach that tries every possible integer translation and evaluates the quality of a match using the squared error norm (the sum of the squared pixel differences). You can set the `maxOffset` to 15 pixels.  \n",
    "2. Align each image to the first image and denoise by averaging all of the aligned images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables:\n",
    "\n",
    "- Python code to align noisy images\n",
    "- Aligned and de-noised average image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "img_list = sorted(glob('./denoising/*.png'))\n",
    "img_ref = plt.imread(img_list[0])\n",
    "\n",
    "# TODO: use a for-loop to load remaining images and check data types\n",
    "\n",
    "def align_imgs(img1, img2, maxOffset):\n",
    "    # TODO: use nested for-loops to compute the error for every possible\n",
    "    # combination of x and y offsets, up to the maxOffset in each dimension\n",
    "    # The function np.roll may be helpful for shifting the images relative to each other\n",
    "    # For each offset, compute the squared error norm\n",
    "    return best_offset, best_error\n",
    "\n",
    "def combine_imgs(img_list, offset_list):\n",
    "    # TODO: for each image, shift it by the corresponding offset from align_imgs\n",
    "    # Compute the mean across all offset images\n",
    "    # Note that you do not need to make an array containing all images\n",
    "    # It is computationally cheaper to add the weighted pixel values to a running sum image\n",
    "    return composite_img\n",
    "\n",
    "# TODO: for each image in img_list, compute best_offset using align_imgs\n",
    "# Use the resulting offset list to generate a composite image\n",
    "\n",
    "plt.axis('off')\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(composite_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: White balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before WB\n",
    "\n",
    "![WB Teaser](white_balance/input.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After WB\n",
    "\n",
    "![WB Teaser](https://raw.githubusercontent.com/W281/fileRepository/main/Assignments/Assignment_2/white_balance/output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Color constancy is one of the most amazing features of the human visual system. When we look at objects under different illuminations, their colors stay relatively constant. This helps humans to more easily identify objects under varying illuminations. A similar behavior is highly desirable in digital still and video cameras. This is achieved via white balancing, typically employed in a digital camera's imaging pipeline to adjust the coloration of images captured under different illuminations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description \n",
    "You will implement two methods for white balancing, gray-world and white-patch, described below. The two images above show before and after white balancing using the gray-world assumption. You can test your white balancing code using this example image. Images which are white-balanced using the white-patch method will have a different appearance depending on the patch selected.\n",
    "\n",
    "1. One simple technique for white balancing is based on the gray-world assumption. This assumption argues that the average reflectance of a scene is achromatic. In other words, the mean of the red, green, and blue channels in a given scene should be roughly equal. We will implement this white balancing technique. Write a function to automatically white balance an image using the gray-world assumption. You should multiply each color channel by a scale factor so that the resulting mean of each of the three color channels is the same and equal to the average value of the green channel of the input image.\n",
    "\n",
    "2. Another method for white balancing uses a white-patch in the image. In this method, the user manually selects an image region which is supposed to be white but looks colored due to the scene illumination. As above, we will scale each color channel by a factor so that the average color of the selected region becomes white. Write a python code to implement the white-patch balancing method. Your code should take in a location in the image and use a fixed-sized region around that point to compute the target white point.\n",
    "\n",
    "In both cases, you will need to account for pixel values that fall outside the displayable range after transformation. You should clip these values rather than scaling them.\n",
    "\n",
    "Please specify the region that you selected in your code for 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables:\n",
    "\n",
    "- Python functions for gray-world and white-patch white balancing\n",
    "- Output images after white balancing the image `white_balance/input.png` using both gray-world and white-patch methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "input_img = plt.imread('./white_balance/input.png')\n",
    "# TODO: check img data type\n",
    "\n",
    "def gray_world(input_img):\n",
    "    # TODO: get average green value\n",
    "    # compute scale factor for red and blue\n",
    "    # rescale red and blue channels\n",
    "    return wb_img\n",
    "\n",
    "def white_patch(input_img):\n",
    "    # TODO: get point(s) from user\n",
    "    # compute scale factors for each color channel\n",
    "    # rescale each color channel\n",
    "    return wb_img\n",
    "\n",
    "gray_world_output = gray_world(input_img)\n",
    "white_patch_output = white_patch(input_img)\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.imshow(gray_world_output)\n",
    "plt.xticks(ticks=[])\n",
    "plt.yticks(ticks=[])\n",
    "plt.title('Gray-World')\n",
    "plt.show()\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.imshow(white_patch_output)\n",
    "plt.xticks(ticks=[])\n",
    "plt.yticks(ticks=[])\n",
    "plt.title('White-Patch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgments\n",
    "This assignment is based on an assignment for Computational Aspects of Digital Photography class by Prof. Wojciech Jarosz at Dartmouth College."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6b4222e535703a1931872bc74813693be38bf05b66176937661c9390e20f284"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
